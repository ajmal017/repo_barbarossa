

(PAGE 21) A very good demostration of how to speed up vector operations using numpy tricks.

(PAGE 30) How to use conda to create seperate environment and install different versions there.

(PAGE 32) Python Quant Platform seems like a potentially useful way of deploying python via web browser.

(PAGE 36) Different interfaces to use ipython.

(PAGE 43) Markdown an latex format in ipython.

(PAGE 44) Time and prun functions magic functions seem useful

(PAGE 63) The majority of numpy code is implemented in C or Fortran, which makes NumPy, when used in the right way,
faster than pure Python.

(PAGE 72) numpy.where seems to be a useful tool for logical indexing.

(PAGE 86) When parsing string objects, consider using regular expressions, which can bring both convenience and performance
to such operations.

(PAGE 107) Depending on the specific shape of the data structures, care should be taken with regard to the memory layout of arrays.
Choosing the right approach here can speed up code execution by a factor of two or more.

(PAGE 177) Pickle stores objects according to first in, first out (FIFO) principle. There is one major problem with this: there is no
meta-information available to the user to know beforehand what is stored in a pickle file. A sometimes helpful workaround is to not
store single objects, but a dict object containing all the other objects. This approach, however, requires us to write and read all 
objects at once. This is a compromise one can probably live with in many circumstances given the much higher convenience it brings along.

(PAGE 183) Writing and reading NumPy arrays as a way of data storage and retrieval is much, much faster compared to SQL databases or 
using the standard pickle library for serialization.

(PAGE 188) Instead of writing and reading to SQL database reading and writing from pandas tables might be a faster alternative.

(PAGE 200) SQL-based (i.e., relational) databases have advantages when it comes to complex data structures that exhibit lots of
relations between single objects/tables. This might justify in some circumstances their performance disadvantage over pure Numpy
ndaray-based or pandas DataFrame-based appraoches. However, many application areas in finance or science in general, can succeed
with a mainly array-based data modeling approach. In these cases, huge performance improvements can be realized by making use of
native Numpy I/O capabilities, a combination of NumpPy and PyTables capabilities, or of the pandas approach via HDF5-based stores.

(PAGE 201) The out-of-memory calculation of the numerical expression with PyTables takes roughly 1.5 minutes on standard hardware.
The same task executed in-memory (using the numexpr library) takes about 4 seconds, while reading the whole data set from disk takes
just over 5 seconds.This value is from an eight-core server with enough memory (in this particular case, 64 GB of RAM) and an SSD drive.
Therefore, scaling up hardware and applying different implementation approaches might significantly influence performance.



[PAGE 212]