

(PAGE 21) A very good demostration of how to speed up vector operations using numpy tricks.

(PAGE 30) How to use conda to create seperate environment and install different versions there.

(PAGE 32) Python Quant Platform seems like a potentially useful way of deploying python via web browser.

(PAGE 36) Different interfaces to use ipython.

(PAGE 43) Markdown an latex format in ipython.

(PAGE 44) Time and prun functions magic functions seem useful

(PAGE 63) The majority of numpy code is implemented in C or Fortran, which makes NumPy, when used in the right way,
faster than pure Python.

(PAGE 72) numpy.where seems to be a useful tool for logical indexing.

(PAGE 86) When parsing string objects, consider using regular expressions, which can bring both convenience and performance
to such operations.

(PAGE 107) Depending on the specific shape of the data structures, care should be taken with regard to the memory layout of arrays.
Choosing the right approach here can speed up code execution by a factor of two or more.

(PAGE 177) Pickle stores objects according to first in, first out (FIFO) principle. There is one major problem with this: there is no
meta-information available to the user to know beforehand what is stored in a pickle file. A sometimes helpful workaround is to not
store single objects, but a dict object containing all the other objects. This approach, however, requires us to write and read all 
objects at once. This is a compromise one can probably live with in many circumstances given the much higher convenience it brings along.

(PAGE 183) Writing and reading NumPy arrays as a way of data storage and retrieval is much, much faster compared to SQL databases or 
using the standard pickle library for serialization.

(PAGE 188) Instead of writing and reading to SQL database reading and writing from pandas tables might be a faster alternative.

(PAGE 200) SQL-based (i.e., relational) databases have advantages when it comes to complex data structures that exhibit lots of
relations between single objects/tables. This might justify in some circumstances their performance disadvantage over pure Numpy
ndaray-based or pandas DataFrame-based appraoches. However, many application areas in finance or science in general, can succeed
with a mainly array-based data modeling approach. In these cases, huge performance improvements can be realized by making use of
native Numpy I/O capabilities, a combination of NumpPy and PyTables capabilities, or of the pandas approach via HDF5-based stores.

(PAGE 201) The out-of-memory calculation of the numerical expression with PyTables takes roughly 1.5 minutes on standard hardware.
The same task executed in-memory (using the numexpr library) takes about 4 seconds, while reading the whole data set from disk takes
just over 5 seconds.This value is from an eight-core server with enough memory (in this particular case, 64 GB of RAM) and an SSD drive.
Therefore, scaling up hardware and applying different implementation approaches might significantly influence performance.

(PAGE 212) There has been some changes in the way Ipython.parralel works. First of all:
from IPython.parallel import Client -> from ipyparallel import Client
Also the following needs to be done in command line:
pip install ipyparallel
ipcluster nbextension enable

(PAGE 214) Ipyparallel can lead to an almost linear scaling of performance with the number of cores available.

(PAGE 215) Multiprocessing can also be used to parallelize within a local machine.

(PAGE 218) With numba you can compile python functions that you've written. This might lead to faster results than 
numpy vectorization.

(PAGE 225) Cython and cythonmagic can be used if one is after compiled c code speed.

(PAGE 226) There is one financial field that can benefit strongly from the use of a GPU: Monte Carlo simulation and 
(pseudo) random number generation in particular.

(PAGE 229) The overhead of GPU is too large for low workloads however for heavy workloads GPU has speed advantage and scales well.

(PAGE 283) Simulation of a stochastic volatility model.

(PAGE 351) Using PyMC3 runs a MCMC algorithm to calculate regression coefficients that are random walk.

(PAGE 417) Bokeh shines when it comes to real-time visualization of, for example, high-frequency financial data.

(PAGE 418) They have an example where they receive real time high frequency forex data from OANDA and plot it real time with Bokeh.

(Page 422) They have an example where they receive real-time data from Netfonds (a Norwegian Broker) and plot it with Bokeh.

(Page 434) They design a webpage using Flask and SQLite3.

(Page 465) A major advantage of object-oriented modeling approach is, for example, that instances of the constant_short_rate class
can live in multiple environments. Once the instance is updated--for example, when a new constant short rate is set--all the instances
of the market_environment class containing that particular instance of the discounting class will be updated automatically.

(PAGE 504) They use Longstaff-Schwartz model to price american options.

[DONE]










