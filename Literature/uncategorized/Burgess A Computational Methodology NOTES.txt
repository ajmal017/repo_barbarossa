
(PAGE 35) Looking at Auto-Correlation function is a good way to detect predictability.

(PAGE 37) Stationarity can be tested with Dickey-Fuller and Augmented-Dickey-Fuller tests. 

(PAGE 39) Variance ratio test can be used to detect devieations from random walk.

(PAGE 40) In order to correct for spurious mean-reversion which is induced by the cointegrating
regression itself, the stationarity tests used in the identification of cointegrating relationships
are modified versions of standard tests for stationarity.

(PAGE 55) Adding appropropriate complexity to a model will reduce the error component which is due to
model bias, but at the price of increasing the error component which is due to model variance. 
The minimal error, and hence optimal performance, will lie at a particular level of complexity which achieves
the best balance of bias against variance.

(PAGE 66) Reinforcement learning offers a systematic framework for optimising trading systems in terms of the
actual trading performance or "Payoff" which they produce, rather than in terms of statistical measures of
performance accuracy which my not necessarily translate directly into trading performance.

(PAGE 68) Some interesting literature review is covered through page 75

(PAGE 76) Papers about combinations

(PAGE 78) For Cac/Dax, Dicky-Fuller fails to reject random walk although the variance-ratio test and
the performance of a simple implicit statistical arbitrage trading rule strongly rejects random walk.

(PAGE 79) The most straightforward way to exploit statistical arbitrage opportunities in the form of
temporary deviations between assets that are correlated in the long term would simply be to buy the
underpriced asset and sell the overpriced asset whenever a sufficiently large mispricing occurs.
However such an approach incurs unquantified risks in that short term dynamics and exogenous factors
may be acting to increase the deviation, leading to short term loses or "draw downs" before the longer
term relationship is restored. A more principled approach is to use the "mispricing" information as
the basis of a predictive model of the relative returns on the two assets.

(PAGE 72) Introducing non linear models seem to improve the performance quite q bit for a simple cointegration
approach.

(PAGE 87) A disadvantage of cointegration is the low-power of standard unit root cointegration tests at 
detecting predictable components. This weakness of cointegration methodology may in principle be overcome by
replacing the unit root tests with more powerful tests based on concepts such as the variance ratio, however
the distribution statistics for such tests is not known and certainly would be expected to be nonstandard
due to biases induced by the cointegrating regression.

(PAGE 97) Rather than imposing the strict requirement of stationarity, we instead employ a broader range of
tests which are capable of identifying both trending and mean-reverting behavior and which are robust to the
presence of nonstationary component in the time-series dynamics.

(page 98) Tests based on the autocorrelation function are most appropriate at detecting short-term effects such 
as momentum or short-term correction effects. In contrast, unit-root tests and standard variance ratio tests
are most appropriate for observing long term effects, such as distinguishing between stationary and nonstationary 
behavior. Tests based on the overall shape of the variance ratio profile are capable of recognising combinations
of short-term and long-term dynamics which are particulary amenable to statistical arbitrage 

(PAGE 100) Formula for implicit statistical arbitrage strategy.

(PAGE 101) Model free variable selection using nonparametric regression.

(PAGE 103) The methodology which we have developed avoids the problem of excessive bias by using flexible neural
network models and avoids excessive estimation variance by using statistical significance testing to control
the specification of the neural network models.

(PAGE 104) Model selection algorithms are explained.

(PAGE 106) Formula for conditional statistical arbirage strategy.

(PAGE 107) The advantage of employing statistical arbitrage strategies based upon flexible forecasting models 
is that  other deterministic components of the dynamics can be accounted for, in addition to the basic tendency
of mean-reversion. By capturing these additional components in the forecasting model it may be possible both
to decrease risk and increase profitability. A decrease in risk may occur because otherwise unfavorable effects,
such as a short-term momentum acting to temporarily increase the mispricing, can be accounted for in the model forecasts.
An increase in profits may occur because the overall accuracy of the forecasting model will improve due to capturing 
a larger part of the deterministic component in the underlying dynamics.

(PAGE 113) The key elements of the specification of each of the three components of a statistical arbitrage model
(mispricing model, forecasting model and trading rule) are grouped together as a set of "meta parameters". The "fitness"
of the set of meta-parameters is evalueated in terms of the actual trading performance of the overall model, thus
serving to avoid the criterion risk which would arise from seperate optimisation of the different model components.
This process can be seen as avoiding the forecasting bottleneck by allowing feedback from the trading performance to 
be passed back to the other model components, thus closing the modelling loop and allowing joint optimisation of all
model components with respect to a single set of criteria.

(PAGE 116) Whilst the object of cointegration testing is to identify combinations of time-series which are stationary,
our statistical arbitrage modelling is concerned with the more general case of combinations of time-series which simply
contain a predictable component. This generalisation of the cointegration framework is both necessary and useful because
the requirement for strict stationarity is over-restrictive and would cause many potential opportunities for statistical
arbitrage to be overlooked.

(PAGE 121) Definition of a syntetic asset model.

(PAGE 127) Aside from cointegration, once can use factor models and PCA. However the author is focusing on cointegration.

(PAGE 137) One can use Kalman Filter to estimate time varying cointegration vector.

(PAGE 140) 400 daily observations are used to select 5 stocks that are the best candidates to replicate a target asset.

(PAGE 146) The reason why standard cointegration tests will in some cases fail to identify potential opportunities 
for statistical arbitrage is simple: it is not the purpose for which the tests are designed. Whilst the object of 
cointegration testing is to identify combinations of time-series which are stationary, statistical arbirage modelling
is concerned with the more general case of combinations of time-series which contain a stationary component, or indeed
any other deterministic component.

(PAGE 147) The Box-Lyung statistic (Q-BL)

(PAGE 147) Unit Root Tests for Stationarity: Dickey-Fuller (DF) and Cointegrating Regression Durbin-Watson (CRDW)

(PAGE 166) Of the variance ratio projections, only the first principal components turn out to be substantially 
correlated with any of the standard tests. Not only are they correlated with each other (0.562), but also with 
the basic variance ratio statistics and, in the case of VP100c1, the stationarity tests of DW and CRDW. From this
we can take two main messages: the first principal components of the variance ratio profiles are perhaps the best
tests overall, and the subsequent principal components can be used to identify classes of potentially predictable
dynamics which are likely to be completely missed by standard predictability tests-be they stationarity tests,
autocorrelation tests or even variance ratio tests.

(PAGE 167) Regression induces a certain amount of spurious "mean-reversion" in the residuals and the impact of this
on the distribution of the test statistics must be corrected if they are to detect truly deterministic components
rather than spurious artefacts of the construction methodology itself. A further complication arises in the case of
the extended stepwise methodology for use in high-dimensional problems, because the "selection bias" inherent in 
choosing the best m out of n>m regressors must also be accounted for.

(PAGE 168) The proposed solution is for the above mentioned probelms is to do Monte Carlo to calculate critical values
from running regressions.

(PAGE 172) So we generate the appropirate  emprical distributions of our test statistics prior to performing the modelling
procedure itself, and such that the experimental parameters (number of regressors, size of the pool from which the regressors
are chosen, and sample size) match those which will be used in the modelling procedure. This approach is rather computationally
intensive, given that the number of simulations required is of the order of 1000s or 10000s depending on the accuracy required,
but is quite within the capabilities of standard PCs.

(PAGE 177) Implicit statistical arbitrage (ISA) trading strategies described.

(PAGE 182) The raw insample profit, ISAPROF(IS), generalises less well to out-of-sample profit (correlation of %28) than does the 
risk-adjusted insample profit, ISASR(IS) (correlation to out-of-sample profit = 38%) suggesting that the best indicator of
continued profitability is the consistency of the in-sample performance rather than the mere magnitude of profits.

(PAGE 183) Sharp ratio's will be improved by choosing models with better in sample perforamances.

(PAGE 184) A general rule of thumb amongst practitioners when evaluating Sharpe Ratio statistics for trading models, is that 
(annualised) values of below 1 are uninteresting, between 1 and 2 indicates that a model shows promise, but only values of 2
or more are considered noteworthy.

(PAGE 185) Sharp ratios will improve by combining models in a portfolio, (notice models here means simple different target assets in
cointegrating regression or different number of regressors)

(PAGE 188) When the trading signal varies with the square of the mispricing, sharpe ratio increases.

(PAGE 194) Kalman filter implementation helps to improve the consistency of German-France index pair trading.
There's an optimal level of adaptability that makes it the most profitable.

(PAGE 211) The lack of statistical significance measures that can be applied on an insample basis reduces the effectiveness of
neural modelling in situations where only small samples of noisy data are available. In such cases, leaving data aside for 
purposes of selection or validation may seriously degrade the quality of the model fit itself.

(PAGE 213) Equivalent kernel representation of linear regression

(PAGE 218) The representational power of neural networks arises from an ability to parsimoniously exploit the degrees of freedom
within the model, in a way which is in some sense optimally related to local properties of the data itself, rather than according
to some external global parameterisation such as "bandwidth" parameters used in kernel smoothing techniques of nonparametric statistics.

(PAGE 223) Basic methodology for detecting potentially nonlinear relationships.


[PAGE 225]