
(PAGE 35) Looking at Auto-Correlation function is a good way to detect predictability.

(PAGE 37) Stationarity can be tested with Dickey-Fuller and Augmented-Dickey-Fuller tests. 

(PAGE 39) Variance ratio test can be used to detect devieations from random walk.

(PAGE 40) In order to correct for spurious mean-reversion which is induced by the cointegrating
regression itself, the stationarity tests used in the identification of cointegrating relationships
are modified versions of standard tests for stationarity.

(PAGE 55) Adding appropropriate complexity to a model will reduce the error component which is due to
model bias, but at the price of increasing the error component which is due to model variance. 
The minimal error, and hence optimal performance, will lie at a particular level of complexity which achieves
the best balance of bias against variance.

(PAGE 66) Reinforcement learning offers a systematic framework for optimising trading systems in terms of the
actual trading performance or "Payoff" which they produce, rather than in terms of statistical measures of
performance accuracy which my not necessarily translate directly into trading performance.

(PAGE 68) Some interesting literature review is covered through page 75

(PAGE 76) Papers about combinations

(PAGE 78) For Cac/Dax, Dicky-Fuller fails to reject random walk although the variance-ratio test and
the performance of a simple implicit statistical arbitrage trading rule strongly rejects random walk.

(PAGE 79) The most straightforward way to exploit statistical arbitrage opportunities in the form of
temporary deviations between assets that are correlated in the long term would simply be to buy the
underpriced asset and sell the overpriced asset whenever a sufficiently large mispricing occurs.
However such an approach incurs unquantified risks in that short term dynamics and exogenous factors
may be acting to increase the deviation, leading to short term loses or "draw downs" before the longer
term relationship is restored. A more principled approach is to use the "mispricing" information as
the basis of a predictive model of the relative returns on the two assets.

(PAGE 72) Introducing non linear models seem to improve the performance quite q bit for a simple cointegration
approach.

(PAGE 87) A disadvantage of cointegration is the low-power of standard unit root cointegration tests at 
detecting predictable components. This weakness of cointegration methodology may in principle be overcome by
replacing the unit root tests with more powerful tests based on concepts such as the variance ratio, however
the distribution statistics for such tests is not known and certainly would be expected to be nonstandard
due to biases induced by the cointegrating regression.

(PAGE 97) Rather than imposing the strict requirement of stationarity, we instead employ a broader range of
tests which are capable of identifying both trending and mean-reverting behavior and which are robust to the
presence of nonstationary component in the time-series dynamics.

(page 98) Tests based on the autocorrelation function are most appropriate at detecting short-term effects such 
as momentum or short-term correction effects. In contrast, unit-root tests and standard variance ratio tests
are most appropriate for observing long term effects, such as distinguishing between stationary and nonstationary 
behavior. Tests based on the overall shape of the variance ratio profile are capable of recognising combinations
of short-term and long-term dynamics which are particulary amenable to statistical arbitrage 

(PAGE 100) Formula for implicit statistical arbitrage strategy.

(PAGE 101) Model free variable selection using nonparametric regression.

(PAGE 103) The methodology which we have developed avoids the problem of excessive bias by using flexible neural
network models and avoids excessive estimation variance by using statistical significance testing to control
the specification of the neural network models.

(PAGE 104) Model selection algorithms are explained.

(PAGE 106) Formula for conditional statistical arbirage strategy.

(PAGE 107) The advantage of employing statistical arbitrage strategies based upon flexible forecasting models 
is that  other deterministic components of the dynamics can be accounted for, in addition to the basic tendency
of mean-reversion. By capturing these additional components in the forecasting model it may be possible both
to decrease risk and increase profitability. A decrease in risk may occur because otherwise unfavorable effects,
such as a short-term momentum acting to temporarily increase the mispricing, can be accounted for in the model forecasts.
An increase in profits may occur because the overall accuracy of the forecasting model will improve due to capturing 
a larger part of the deterministic component in the underlying dynamics.

(PAGE 113) The key elements of the specification of each of the three components of a statistical arbitrage model
(mispricing model, forecasting model and trading rule) are grouped together as a set of "meta parameters". The "fitness"
of the set of meta-parameters is evalueated in terms of the actual trading performance of the overall model, thus
serving to avoid the criterion risk which would arise from seperate optimisation of the different model components.
This process can be seen as avoiding the forecasting bottleneck by allowing feedback from the trading performance to 
be passed back to the other model components, thus closing the modelling loop and allowing joint optimisation of all
model components with respect to a single set of criteria.

(PAGE 116) Whilst the object of cointegration testing is to identify combinations of time-series which are stationary,
our statistical arbitrage modelling is concerned with the more general case of combinations of time-series which simply
contain a predictable component. This generalisation of the cointegration framework is both necessary and useful because
the requirement for strict stationarity is over-restrictive and would cause many potential opportunities for statistical
arbitrage to be overlooked.

(PAGE 121) Definition of a syntetic asset model.

(PAGE 127) Aside from cointegration, once can use factor models and PCA. However the author is focusing on cointegration.

(PAGE 137) One can use Kalman Filter to estimate time varying cointegration vector.

(PAGE 140) 400 daily observations are used to select 5 stocks that are the best candidates to replicate a target asset.

(PAGE 146) The reason why standard cointegration tests will in some cases fail to identify potential opportunities 
for statistical arbitrage is simple: it is not the purpose for which the tests are designed. Whilst the object of 
cointegration testing is to identify combinations of time-series which are stationary, statistical arbirage modelling
is concerned with the more general case of combinations of time-series which contain a stationary component, or indeed
any other deterministic component.

(PAGE 147) The Box-Lyung statistic (Q-BL)

(PAGE 147) Unit Root Tests for Stationarity: Dickey-Fuller (DF) and Cointegrating Regression Durbin-Watson (CRDW)

(PAGE 166) Of the variance ratio projections, only the first principal components turn out to be substantially 
correlated with any of the standard tests. Not only are they correlated with each other (0.562), but also with 
the basic variance ratio statistics and, in the case of VP100c1, the stationarity tests of DW and CRDW. From this
we can take two main messages: the first principal components of the variance ratio profiles are perhaps the best
tests overall, and the subsequent principal components can be used to identify classes of potentially predictable
dynamics which are likely to be completely missed by standard predictability tests-be they stationarity tests,
autocorrelation tests or even variance ratio tests.

(PAGE 167) Regression induces a certain amount of spurious "mean-reversion" in the residuals and the impact of this
on the distribution of the test statistics must be corrected if they are to detect truly deterministic components
rather than spurious artefacts of the construction methodology itself. A further complication arises in the case of
the extended stepwise methodology for use in high-dimensional problems, because the "selection bias" inherent in 
choosing the best m out of n>m regressors must also be accounted for.

(PAGE 168) The proposed solution is for the above mentioned probelms is to do Monte Carlo to calculate critical values
from running regressions.

(PAGE 172) So we generate the appropirate  emprical distributions of our test statistics prior to performing the modelling
procedure itself, and such that the experimental parameters (number of regressors, size of the pool from which the regressors
are chosen, and sample size) match those which will be used in the modelling procedure. This approach is rather computationally
intensive, given that the number of simulations required is of the order of 1000s or 10000s depending on the accuracy required,
but is quite within the capabilities of standard PCs.

(PAGE 177) Implicit statistical arbitrage (ISA) trading strategies described.

(PAGE 182) The raw insample profit, ISAPROF(IS), generalises less well to out-of-sample profit (correlation of %28) than does the 
risk-adjusted insample profit, ISASR(IS) (correlation to out-of-sample profit = 38%) suggesting that the best indicator of
continued profitability is the consistency of the in-sample performance rather than the mere magnitude of profits.

(PAGE 183) Sharp ratio's will be improved by choosing models with better in sample perforamances.

(PAGE 184) A general rule of thumb amongst practitioners when evaluating Sharpe Ratio statistics for trading models, is that 
(annualised) values of below 1 are uninteresting, between 1 and 2 indicates that a model shows promise, but only values of 2
or more are considered noteworthy.

(PAGE 185) Sharp ratios will improve by combining models in a portfolio, (notice models here means simple different target assets in
cointegrating regression or different number of regressors)

(PAGE 188) When the trading signal varies with the square of the mispricing, sharpe ratio increases.

(PAGE 194) Kalman filter implementation helps to improve the consistency of German-France index pair trading.
There's an optimal level of adaptability that makes it the most profitable.

(PAGE 211) The lack of statistical significance measures that can be applied on an insample basis reduces the effectiveness of
neural modelling in situations where only small samples of noisy data are available. In such cases, leaving data aside for 
purposes of selection or validation may seriously degrade the quality of the model fit itself.

(PAGE 213) Equivalent kernel representation of linear regression

(PAGE 218) The representational power of neural networks arises from an ability to parsimoniously exploit the degrees of freedom
within the model, in a way which is in some sense optimally related to local properties of the data itself, rather than according
to some external global parameterisation such as "bandwidth" parameters used in kernel smoothing techniques of nonparametric statistics.

(PAGE 223) Basic methodology for detecting potentially nonlinear relationships.

(PAGE 225) Detecting nonlinear relationship comes down to an f test of regression results.

(PAGE 226) Non linear relationship test needs to be adjusted when ridge regression is used.

(PAGE 227) Non linear relationship test needs to be adjsted when neural networks are used.

(PAGE 227) Linear, Polynomial, Bin-Smoother, Radial Basis Function and MLP Network

(PAGE 234) Among the single estimator tests the most powerful ones are the Radial Basis and MLP.
On a general level the results indicate that, for sample sizes in the order of a few hundred 
observations, it is relatively easy to detect significant relationships when they account
for as much as 0.33 of the total variance of the target time-series. When the magnitude of the
pedictable component is of the order of 0.10 it is still possible to identify relationships with
95% plus effectiveness, but typically only when the nature of the estimator closely reflects
that of he underlying relationship. In cases where the predictable component is of the order of
0.033 the situation is much more difficult, with an average identification rate of only around 30%.

(PAGE 234) Joint tests which are conducted by applying all five types of estimator to the data, and
taking the most significant statistic as an indicator of a presence of a relationship are substantially
more powerful than the single-estimator tests, partiuclarly for the lower levels of predictability.

(PAGE 236) False positive for non linearity tests (these are partial F-Tests(page 230)) 
increases as the magnitude of the predictable component decreaes, indicating that at low levels of predictability 
it is particularly difficult to distinguish between linear and nonlinear effects.

(PAGE 237) For detecting nonlinear relationships the two types of neural network model perform similarly well, and
substantially better than both the polynomial and, least effective of all, the bin-smoother. The overall detection
rate is highest for the joint statistic; however, this improved performance does come at the price of a slight increase
in the corresponding false positive rate.

(PAGE 237) For datasets of a typical size of a few hundred observations the practical limit of effectiveness for forecasting
models is when the potentially predictable component accounts for somewhere around 0.033 of the total variance.
Furthermore, in cases close to this borderline it may be preferable to increase the p-value in the acknowledgement of the 
uncertainity in the variable selection procedure, and to err on the side of caution even at the price of risking the inclusion
of insignificant variables in the model estimation procedure proper.

(PAGE 244) The objective of our estimation procedures is to generate models which are neither too low in complexity (overly biased)
nor too high in complexity (overly susceptible to variance) whilst simultaneously ensuring that the complexity is exploited
in an appropriate manner. The essence of our methodolgy is to avoid the problem of excessive bias by using flexible neural
network models and avoid excessive estimation variance by using statistical testing to control the specification of the
neural network models.

(PAGE 256) Details of the regularisation-based algorithm -- there might be some typos here.

(PAGE 258) Details of the constructive algorithm for neurl model estimation.

(PAGE 260) Details of the deconstructive algorithm for neural model estimation.

(PAGE 265) The deconstructive algorithm is comparatively robust to the increasing dimensionality because its variable 
selection procedure is capable of identifying structure involving arbitrarily high-order interaction effects between
the variables.

(PAGE 274-276) Description of candidate variables.

(PAGE 279) Over the set of 90 synthetic assets, the total number of selected variables was 121 (mispricings) +
100 (direct exogenous) + 203 (indirect exogenous) = 424, an average of 4.7 per model.

(PAGE 295) The deconstructive version of the neural estimation methodology produces the most consistent risk-adjusted
performance, corresponding to a 50% improvement over a set of linear models.

(PAGE 299) All model selection criteria based upon model fit can be expressed as functions of two measures: the amount
of insample variance which is captured by the model, and the number of degrees of freedom absorbed by the model.

(PAGE 312) The inclusion of transaction costs at 50 basis points (0.5%) creates a sufficient downward bias in performance
that the simple (equally weighted) model combination strategy is no longer successful. However, a subset selection 
criterion based on performance during the first subperiod achieves consistently profiable performance. 

(PAGE 315) The more sophisticated "portfolio of models" approach improves performance by allowing the correlation structure
of the model performances to be exploited, and has the additional advantage of being applicable on an "after cost" basis.



[PAGE 324]

